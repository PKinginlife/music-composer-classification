{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c5a1657",
      "metadata": {
        "id": "3c5a1657"
      },
      "source": [
        "# Music Composer Classification Using CNN and LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dba6961",
      "metadata": {
        "id": "4dba6961"
      },
      "source": [
        "## Introduction\n",
        "  \n",
        "  This project aims to classify music composers; Bach, Beethoven, Chopin, and Mozart—based on their musical pieces using deep learning techniques. The core approach involves extracting features from MIDI files and using CNN and LSTM models to predict the composer.\n",
        "  \n",
        "\n",
        "  ---------------------\n",
        "\n",
        "## Project Goal\n",
        "Use deep learning to accurately identify the composer of a given piece of music.\n",
        "\n",
        "## Objective:\n",
        "Develop a model to predict the composer from the four specified: Bach, Beethoven, Chopin, and Mozart, leveraging CNN and LSTM architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb564e9",
      "metadata": {
        "id": "2cb564e9"
      },
      "source": [
        "## External Libraries and Frameworks\n",
        "\n",
        "| Library/Tool            | Purpose                                            | Reference / Installation Source                                                                         |\n",
        "| ----------------------- | -------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
        "| **TensorFlow**          | Deep learning framework                            | `pip install tensorflow` <br> [https://www.tensorflow.org/](https://www.tensorflow.org/)                |\n",
        "| **Librosa**             | Audio processing and feature extraction            | `pip install librosa` <br> [https://librosa.org/](https://librosa.org/)                                 |\n",
        "| **SoundFile**           | Audio file reading                                 | `pip install soundfile` <br> [https://pysoundfile.readthedocs.io/](https://pysoundfile.readthedocs.io/) |\n",
        "| **Tqdm**                | Progress bars                                      | `pip install tqdm` <br> [https://tqdm.github.io/](https://tqdm.github.io/)                              |\n",
        "| **Timidity**            | MIDI to audio conversion                           | `apt-get install -y timidity` (Linux)                                                                   |\n",
        "| **Python Standard Lib** | File operations, JSON handling, subprocess control | [https://docs.python.org/3/standard-library/](https://docs.python.org/3/standard-library/)              |\n",
        "\n",
        "\n",
        "--------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xX9X9ozOvu_r",
      "metadata": {
        "id": "xX9X9ozOvu_r"
      },
      "source": [
        "## Methodology and Code Explanation\n",
        "\n",
        "1. **Data Preparation and Pre-processing**\n",
        "\n",
        "\n",
        "*   Unzipping and organizing dataset: To make raw data accessible for processing, the dataset is unzipped into a working directory in Colab environment.\n",
        "\n",
        "*   Cleaning composer folders:\n",
        "To ensure data consistency and remove irrelevant files before feature extraction, the function `clean_composer_folder` is used to move all files from nested subfolders to a single composer directory, removes non-MIDI files, and deletes empty folders.\n",
        "\n",
        "\n",
        "2. **Feature Extraction**\n",
        "\n",
        "\n",
        "*   Rendering MIDI to audio:\n",
        "The function `render_midi_to_audio` converts MIDI files into audio (WAV) format using timidity, then loads the audio for processing.\n",
        "to convert to raw MIDI files to the required numeric audio signal input format needed for Deep learning models.\n",
        "\n",
        "*   Extracting MFCC features:\n",
        "The `save_mfcc` function loads the audio signal, normalizes length, splits it into segments, and extracts Mel-frequency cepstral coefficients (MFCCs) using librosa. It saves the features and labels in a JSON file.\n",
        "MFCCs are widely used features representing the audio’s frequency spectrum, making them suitable for music classification.\n",
        "\n",
        "\n",
        "3. **Model Development**\n",
        "\n",
        "\n",
        "\n",
        "*   `Loading data` is a function used to load the JSON file containing MFCCs and labels into NumPy arrays for training.\n",
        "\n",
        "*   `Building CNN model`:\n",
        "A CNN model is built using TensorFlow/Keras layers (Conv2D, MaxPooling2D, Dense, Dropout). The model input shape corresponds to the MFCC feature dimensions. CNNs are effective for spatial feature extraction from spectrogram-like inputs (MFCCs).\n",
        "\n",
        "* Training: The model is trained with training and validation splits, optimizing for classification accuracy.\n",
        "\n",
        "4. **Evaluation and Optimization**\n",
        "Evaluation is done through metrics such as accuracy, precision, and recall.\n",
        "\n",
        "Hyperparameter tuning and data augmentation may be applied for improving the model.\n",
        "\n",
        "----------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2988fe76",
      "metadata": {
        "id": "2988fe76"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f59138b",
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c52290",
      "metadata": {
        "id": "36c52290"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "iglFdhGCYsy5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iglFdhGCYsy5",
        "outputId": "6ed7e03d-3bed-4bb5-8739-e55cfe2af0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fgnSyChIbWx1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgnSyChIbWx1",
        "outputId": "3167e1ff-5eee-4816-bb43-12b21a4d719f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted to //content/drive/MyDrive/Aritificial Intelligence/midi-classic-music_unzipped\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to your zip file\n",
        "zip_path = \"/content/drive/MyDrive/Aritificial Intelligence/midi-classic-music.zip\"   # change this to your file path\n",
        "extract_dir = \"//content/drive/MyDrive/Aritificial Intelligence/midi-classic-music_unzipped\"  # where you want to extract\n",
        "\n",
        "# Make sure output directory exists\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Extracted to {extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "56bd2851",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56bd2851",
        "outputId": "21bf6efa-01d5-41c1-9e1f-dbb03f4ce6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bach Copied successfully\n",
            "Chopin Copied successfully\n",
            "Mozart Copied successfully\n",
            "Beethoven Copied successfully\n",
            "Filtering complete. Original dataset removed.\n"
          ]
        }
      ],
      "source": [
        "# Get data folder to reflect selected composers only\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Original dataset\n",
        "root_dir = \"/content/drive/MyDrive/Aritificial Intelligence/midi-classic-music_unzipped/midiclassics\"\n",
        "\n",
        "# New filtered dataset folder\n",
        "filtered_dir = \"/content/drive/MyDrive/Aritificial Intelligence/midclassics_filtered\"\n",
        "\n",
        "# Composers to keep\n",
        "keep_composers = {\"Bach\", \"Beethoven\", \"Chopin\", \"Mozart\"}\n",
        "\n",
        "# Checks and ensure the filtered dataset folder exists\n",
        "os.makedirs(filtered_dir, exist_ok=True)\n",
        "\n",
        "# Loop through composers in root_dir\n",
        "for composer in keep_composers:\n",
        "    src_folder = os.path.join(root_dir, composer)\n",
        "    dest_folder = os.path.join(filtered_dir, composer)\n",
        "\n",
        "    if os.path.exists(src_folder):\n",
        "        shutil.copytree(src_folder, dest_folder)\n",
        "        print(f\"{composer} Copied successfully\")\n",
        "    else:\n",
        "        print(f\"Warning: {composer} folder not found in {root_dir}\")\n",
        "\n",
        "# Delete the original dataset folder after copying\n",
        "shutil.rmtree('/content/drive/MyDrive/Aritificial Intelligence/midi-classic-music_unzipped')\n",
        "\n",
        "print(\"Filtering complete. Original dataset removed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fff50e4d",
      "metadata": {
        "id": "fff50e4d"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b83b919",
      "metadata": {
        "id": "4b83b919"
      },
      "outputs": [],
      "source": [
        "from typing import Set\n",
        "\n",
        "def clean_composer_folder(composer_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Clean and flatten a composer's directory.\n",
        "\n",
        "    This function moves all files from subfolders into `composer_dir`,\n",
        "    removes non-.mid files, and deletes any empty subfolders.\n",
        "\n",
        "    Args:\n",
        "        composer_dir (str): Path to the composer's folder.\n",
        "    \"\"\"\n",
        "    # Move files up to composer_dir\n",
        "    for root, _, files in os.walk(composer_dir, topdown=False):\n",
        "        if root == composer_dir:\n",
        "            continue\n",
        "\n",
        "        for file_name in files:\n",
        "            src_file = os.path.join(root, file_name)\n",
        "            dst_file = os.path.join(composer_dir, file_name)\n",
        "\n",
        "            # Rename to avoid overwrite\n",
        "            if os.path.exists(dst_file):\n",
        "                base, ext = os.path.splitext(file_name)\n",
        "                counter = 1\n",
        "                while os.path.exists(\n",
        "                    os.path.join(composer_dir, f\"{base}_{counter}{ext}\")\n",
        "                ):\n",
        "                    counter += 1\n",
        "                dst_file = os.path.join(\n",
        "                    composer_dir, f\"{base}_{counter}{ext}\"\n",
        "                )\n",
        "\n",
        "            shutil.move(src_file, dst_file)\n",
        "\n",
        "    # Remove empty subdirectories\n",
        "    for root, _, _ in os.walk(composer_dir, topdown=False):\n",
        "        if root == composer_dir:\n",
        "            continue\n",
        "        if not os.listdir(root):\n",
        "            os.rmdir(root)\n",
        "\n",
        "    # Delete non-.mid files in composer_dir\n",
        "    for file_name in os.listdir(composer_dir):\n",
        "        file_path = os.path.join(composer_dir, file_name)\n",
        "        if os.path.isfile(file_path) and not file_name.lower().endswith(\".mid\"):\n",
        "            os.remove(file_path)\n",
        "            print(f\"Deleted non-MIDI file: {file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "root_dataset_dir = \"/content/drive/MyDrive/Aritificial Intelligence/midclassics_filtered\"\n",
        "composers = {\"Bach\", \"Beethoven\", \"Chopin\", \"Mozart\"}\n",
        "\n",
        "for composer in composers:\n",
        "    composer_path = os.path.join(root_dataset_dir, composer)\n",
        "    if os.path.exists(composer_path):\n",
        "        print(f\"\\nProcessing {composer} folder...\")\n",
        "        clean_composer_folder(composer_path)\n",
        "    else:\n",
        "        print(f\"Folder for {composer} not found at {composer_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ZkcHrEN0zH",
      "metadata": {
        "id": "65ZkcHrEN0zH"
      },
      "outputs": [],
      "source": [
        "!pip install pretty_midi librosa numpy tqdm\n",
        "!apt-get install -y fluidsynth timidity\n",
        "!pip install soundfile"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JvP4HeNVR0Kd",
      "metadata": {
        "id": "JvP4HeNVR0Kd"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "961BzNPYPfml",
      "metadata": {
        "id": "961BzNPYPfml"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import json\n",
        "import librosa\n",
        "import subprocess\n",
        "import tempfile\n",
        "from typing import Tuple, Optional, Dict, Any\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf  # For reading audio\n",
        "\n",
        "\n",
        "JSON_PATH: str = (\n",
        "    \"/content/drive/MyDrive/Aritificial Intelligence/composer_mfcc_data.json\"\n",
        ")\n",
        "SAMPLE_RATE: int = 22050\n",
        "DURATION: int = 15\n",
        "SAMPLES_PER_TRACK: int = SAMPLE_RATE * DURATION\n",
        "\n",
        "\n",
        "def render_midi_to_audio(\n",
        "    midi_path: str, sample_rate: int = SAMPLE_RATE\n",
        ") -> Tuple[Optional[list], Optional[int]]:\n",
        "    \"\"\"\n",
        "    Render a MIDI file to audio and return the waveform and sample rate.\n",
        "\n",
        "    Args:\n",
        "        midi_path (str): Path to the MIDI file.\n",
        "        sample_rate (int): Desired sample rate for the output audio.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[list], Optional[int]]:\n",
        "            - audio waveform as a list or None if failed\n",
        "            - sample rate or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmpwav:\n",
        "            cmd = [\"timidity\", midi_path, \"-Ow\", \"-o\", tmpwav.name]\n",
        "            subprocess.run(\n",
        "                cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "            )\n",
        "            audio, sr = sf.read(tmpwav.name)\n",
        "\n",
        "            if sr != sample_rate:\n",
        "                audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)\n",
        "                sr = sample_rate\n",
        "\n",
        "            return audio, sr\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Rendering MIDI failed for {midi_path} with error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def save_mfcc(\n",
        "    dataset_path: str,\n",
        "    json_path: str,\n",
        "    n_mfcc: int = 13,\n",
        "    n_fft: int = 2048,\n",
        "    hop_length: int = 512,\n",
        "    num_segments: int = 5\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Extract MFCC features from a dataset of MIDI files and save them to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): Path to dataset with subfolders for each composer.\n",
        "        json_path (str): Path to save the output JSON file.\n",
        "        n_mfcc (int): Number of MFCCs to extract per segment.\n",
        "        n_fft (int): FFT window size.\n",
        "        hop_length (int): Number of samples between successive frames.\n",
        "        num_segments (int): Number of equal segments to split each track into.\n",
        "    \"\"\"\n",
        "    data: Dict[str, Any] = {\n",
        "        \"mapping\": [],\n",
        "        \"mfcc\": [],\n",
        "        \"labels\": []\n",
        "    }\n",
        "\n",
        "    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
        "    expected_num_mfcc_per_segment = math.ceil(num_samples_per_segment / hop_length)\n",
        "\n",
        "    # Loop through all composer folders\n",
        "    for i, (dirpath, _, filenames) in enumerate(os.walk(dataset_path)):\n",
        "        if dirpath != dataset_path:\n",
        "            semantic_label = os.path.basename(dirpath)\n",
        "            data[\"mapping\"].append(semantic_label)\n",
        "            print(f\"\\nProcessing {semantic_label}\")\n",
        "\n",
        "            for file_name in tqdm(filenames, desc=f\"{semantic_label} files\"):\n",
        "                file_path = os.path.join(dirpath, file_name)\n",
        "\n",
        "                if not file_name.lower().endswith((\".mid\", \".midi\")):\n",
        "                    continue\n",
        "\n",
        "                signal, sr = render_midi_to_audio(file_path, sample_rate=SAMPLE_RATE)\n",
        "                if signal is None:\n",
        "                    continue\n",
        "\n",
        "                # Ensure fixed length\n",
        "                if len(signal) < SAMPLES_PER_TRACK:\n",
        "                    signal = librosa.util.fix_length(signal, size=SAMPLES_PER_TRACK)\n",
        "                else:\n",
        "                    signal = signal[:SAMPLES_PER_TRACK]\n",
        "\n",
        "                # Process segments\n",
        "                for s in range(num_segments):\n",
        "                    start_sample = num_samples_per_segment * s\n",
        "                    finish_sample = start_sample + num_samples_per_segment\n",
        "\n",
        "                    if finish_sample > len(signal):\n",
        "                        continue\n",
        "\n",
        "                    mfcc = librosa.feature.mfcc(\n",
        "                        y=signal[start_sample:finish_sample],\n",
        "                        sr=sr,\n",
        "                        n_mfcc=n_mfcc,\n",
        "                        n_fft=n_fft,\n",
        "                        hop_length=hop_length\n",
        "                    ).T\n",
        "\n",
        "                    if len(mfcc) == expected_num_mfcc_per_segment:\n",
        "                        data[\"mfcc\"].append(mfcc.tolist())\n",
        "                        data[\"labels\"].append(i - 1)\n",
        "                        print(f\"{file_path}, segment: {s + 1}\")\n",
        "\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q2_AaipbLrca",
      "metadata": {
        "id": "q2_AaipbLrca"
      },
      "outputs": [],
      "source": [
        "save_mfcc(root_dataset_dir, JSON_PATH, num_segments=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ndZJmwkUhxzN",
      "metadata": {
        "id": "ndZJmwkUhxzN"
      },
      "outputs": [],
      "source": [
        "# Load JSON data\n",
        "import numpy as np\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Aritificial Intelligence/composer_mfcc_data.json\"\n",
        "def load_data(data_path):\n",
        "  with open(data_path, \"r\") as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  X = np.array(data[\"mfcc\"])\n",
        "  y = np.array(data[\"labels\"])\n",
        "  return X, y, data[\"mapping\"]\n",
        "\n",
        "X, y, mapping = load_data(JSON_PATH)\n",
        "print(f\"Data loaded: X.shape={X.shape}, y.shape={y.shape}, classes={mapping}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BzGP6oisqIsh",
      "metadata": {
        "id": "BzGP6oisqIsh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Train/Test/Val split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Add channel dimension for CNN\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_val = X_val[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qBItGoxBngvr",
      "metadata": {
        "id": "qBItGoxBngvr"
      },
      "source": [
        "## Build CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "sW5kBMdLub4Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sW5kBMdLub4Z",
        "outputId": "a20cfa3a-0ee8-4a68-a9d0-c83ca46df4f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6RN79wHEnf4I",
      "metadata": {
        "id": "6RN79wHEnf4I"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    Flatten,\n",
        "    Dense,\n",
        "    Dropout\n",
        ")\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "\n",
        "def build_cnn_model(input_shape: Tuple[int, int, int], num_classes: int) -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Build and compile a CNN model for classification.\n",
        "\n",
        "    Args:\n",
        "        input_shape (Tuple[int, int, int]): Shape of the input data (height, width, channels).\n",
        "        num_classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled CNN model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Conv2D(32, (3, 3), activation=\"relu\"),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation=\"softmax\"),\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# call function to train model\n",
        "model = build_cnn_model(X_train.shape[1:], num_classes=len(mapping))\n",
        "model.summary()\n",
        "\n",
        "history: History = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fl3elBctnojj",
      "metadata": {
        "id": "fl3elBctnojj"
      },
      "source": [
        "## Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rGXrq1tInn7Z",
      "metadata": {
        "id": "rGXrq1tInn7Z"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
